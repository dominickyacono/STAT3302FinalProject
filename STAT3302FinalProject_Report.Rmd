---
title: "STAT3302FinalProject_Report"
author: "Dominick Yacono, Ben Gavie, Jack Kamnikar"
date: "2024-04-05"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#importing libraries
library(alr4)
library(tidyverse)
library(broom)
library(ggplot2)
library(corrplot)

#importing datasets
game_data <- read.csv('NBAData/games.csv')
game_data_player_specific <- read.csv('NBAData/games_details.csv')
```

In this project, we are analyzing the factors that have an affect on the odds of a home team winning on home court in the NBA. 

To conduct our analysis of NBA data, we are utilizing two datasetsâ€”'games.csv' and 'game_details.csv'.  We are mainly interested in what skills are most effective toward affecting the odds of a home team winning. Because this dataset is so large and we have issues handling such large files on our systems, we will limit the data to the 2022 NBA season. 

```{R}
game_data <- game_data[game_data$SEASON == "2022", ]
nrow(game_data)
```
We are working with 542 observations from 2022 season games.

# Exploratory Data Analysis

## Removing Unnecessary Columns

First, let's find which columns in the datasets will not be helpful in our analysis.

### games.csv file
```{r}
head(game_data,5)
```

The 'GAME_ID', 'HOME_TEAM_ID', and 'VISITOR_TEAM_ID' variables will be important to keep since they unique identify each game. Since we are looking at the skill/performance related factors that influence victory, we can remove 'GAME_DATE_EST', 'SEASON', and 'GAME_STATUS_TEXT'. We can also remove 'PTS_home' since it likely is influenced by other factors already in the model like shooting percentages. Also, factors like shooting percentages are more valuable since they describe more specific skills needed rather than simply "make more points".There also appear to be duplicate columns for the home team ID and away team ID, so we will remove these columns. 

```{r}
# Removing columns from game_data
columns_to_keep <- names(game_data)[!names(game_data) %in% c("GAME_DATE_EST", "SEASON", "PTS_home", "GAME_STATUS_TEXT" ,"TEAM_ID_home", "TEAM_ID_away")]
game_data <- subset(game_data, select = columns_to_keep)
game_data
```

### games_details.csv file
```{r}
head(game_data_player_specific,5)
```

We can drop 'TEAM_CITY', 'PLAYER_NAME', 'NICKNAME', 'START_POSITION', 'MIN', and 'TEAM_ABBREVIATION' since these variables do not focus on player skill or performance during games. We can also remove 'PLUS_MINUS' because it contains lots of NULL observations.
```{r}
# Removing columns from game_data
columns_to_keep <- names(game_data_player_specific)[!names(game_data_player_specific) %in% c("TEAM_CITY", "PLAYER_NAME", "NICKNAME", "PLUS_MINUS" ,"TEAM_ABBREVIATION", "START_POSITION", "MIN")]
game_data_player_specific <- subset(game_data_player_specific, select = columns_to_keep)

game_data_player_specific 
```

We will remove 'COMMENT' as well, but first we must filter the data to not include players with comment of "DNP - Coach's Decision".  Players with this comments did not play in the game, so therefore they are not useful in our analysis.

```{r}
#Filtering the game_data_player_specific
game_data_player_specific_filtered <- game_data_player_specific[!game_data_player_specific$COMMENT != "", ]

#Removing "COMMENT" column
columns_to_keep <- names(game_data_player_specific_filtered)[!names(game_data_player_specific_filtered) %in% c("COMMENT")]
game_data_player_specific_filtered <- subset(game_data_player_specific_filtered, select = columns_to_keep)
game_data_player_specific_filtered

game_data_player_specific_filtered
```

## Combining Into One Dataframe

Let's join the tables by GAME_ID

```{r}
# Removing columns from game_data
total_game_data <- merge(x = game_data_player_specific_filtered, y = game_data, by.x = "GAME_ID", by.y = "GAME_ID", all.x = FALSE, all.y = FALSE)
total_game_data <- total_game_data[order(total_game_data$GAME_ID), ] 

total_game_data
```

Some columns between 'games.csv' and 'gamesdata.csv' are shared and cover the same attributes, so let' remove 'FCM', 'FGA', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'REB', and 'AST'. 



## Aggregating The Data

Now that the data has been combined, we must aggregate the player specific data. This will allow us to analyze how the team performed as a whole for each game.

First, we can see that we have 'FGM' (Field goals made) and 'FGA' (Field goals attempted). We can make an aggregated variables out of this. Let's make a variable 

```{R}
# Removing columns from total_game_data
columns_to_keep <- names(total_game_data)[!names(total_game_data) %in% c('FGM', 'FGA', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'REB', 'AST')]
total_game_data <- subset(total_game_data, select = columns_to_keep)

total_game_data
```

We will aggregate the data for just home teams, since this study is examining them.

#### Aggregating The Data For Home Teams
```{R}
# Only obtain players that played for the home team
home_team_stats_aggregated <- filter(total_game_data, TEAM_ID == HOME_TEAM_ID)
home_team_stats_aggregated

#Group by game_id and make aggregate data
home_team_stats_aggregated <- home_team_stats_aggregated %>% group_by(GAME_ID, HOME_TEAM_ID, VISITOR_TEAM_ID, FG_PCT_home, FT_PCT_home, FG3_PCT_home, AST_home, REB_home, HOME_TEAM_WINS) %>%
  summarize(OREB_home = sum(OREB), DREB_home = sum(DREB), STL_home = sum(STL), 
            BLK_home = sum(BLK), TO_home = sum(TO), PF_home = sum(PF), avg_PTS_per_player_home = mean(PTS)) 

#Remove GAME_ID, HOME_TEAM_ID, and VISITOR_TEAM_ID now since we are done bringing the data together 

columns_to_keep <- names(home_team_stats_aggregated)[!names(home_team_stats_aggregated) %in% c("GAME_ID","TEAM_ID", "HOME_TEAM_ID", "VISITOR_TEAM_ID")]
home_team_stats_aggregated <- subset(home_team_stats_aggregated, select = columns_to_keep)

home_team_stats_aggregated 
```

## Univariate Analysis

When conducting regression analysis, it is helpful to know the distributions of the data we will use to model. If the data follows a normal distribution, then there is a better chance the residuals (errors between predicted and actual values) are more likely to remain clustered around zero and maintain a normal distribution. Let's start with analyzing central tendency.

### Analyzing Central Tendency
```{R}
columns_to_keep <- names(home_team_stats_aggregated)[!names(home_team_stats_aggregated) %in% c("HOME_TEAM_WINS")]
home_team_stats_aggregated_no_response <- subset(home_team_stats_aggregated, select = columns_to_keep)

summary(home_team_stats_aggregated_no_response)
```
We can observe from the output above that the means don't deviate much from the medians. This indicates that the unvariate distributions might be approximately normal. Let's look at box plots to observe if there are many outliers in the distributions.

Box plots:
```{R}
column_names <- names(home_team_stats_aggregated_no_response)

for (col in column_names) {
  boxplot(home_team_stats_aggregated_no_response[, col], main = col, xlab = col, ylab = "", notch = FALSE)
}
```
There are a large number of outliers across these box plots. This suggests a wider spread of data points compared to a normal distribution. Let's finally plot the distributions to judge their normality and spread.

### Analyzing Spread

```{R}
column_names <- names(home_team_stats_aggregated_no_response) 

density_plots <- lapply(column_names, function(col) {
  ggplot(home_team_stats_aggregated_no_response, aes_string(x = col)) +
    stat_density(aes(), geom = "area", alpha = 0.5) +  
    labs(title = col, x = col, y = "Density") +
    theme_bw()
})

density_plots

```
We can observe that although there is slight skew amongst most of these, many of them appear to follow relatively normal distributions. 

Let's conduct Q-Q plots to better analyze normality. Q-Q plots compare the quantiles of two distributions, one being a sample data set and the other being an ideal, standard normal distribution. In our context, we will assess how closely our NBA data measurements follow a normal distribution. If the data perfectly follows the normal distribution, the points on the plot will fall along a straight diagonal line

### Q-Q Plots and Normality

```{R}
qqnorm(home_team_stats_aggregated_no_response$FG_PCT_home, main = "QQplot of FG_PCT_Home")
qqline(home_team_stats_aggregated_no_response$FG_PCT_home)

qqnorm(home_team_stats_aggregated_no_response$FT_PCT_home, main = "QQplot of FT_PCT_Home")
qqline(home_team_stats_aggregated_no_response$FT_PCT_home)

qqnorm(home_team_stats_aggregated_no_response$FG3_PCT_home, main = "QQplot of FG3_PCT_Home")
qqline(home_team_stats_aggregated_no_response$FG3_PCT_home)

qqnorm(home_team_stats_aggregated_no_response$AST_home, main = "QQplot of AST_Home")
qqline(home_team_stats_aggregated_no_response$AST_home)

qqnorm(home_team_stats_aggregated_no_response$REB_home, main = "QQplot of REB_Home")
qqline(home_team_stats_aggregated_no_response$REB_home)

qqnorm(home_team_stats_aggregated_no_response$OREB_home, main = "QQplot of OREB_Home")
qqline(home_team_stats_aggregated_no_response$OREB_home)

qqnorm(home_team_stats_aggregated_no_response$DREB_home, main = "QQplot of DREB_Home")
qqline(home_team_stats_aggregated_no_response$DREB_home)

qqnorm(home_team_stats_aggregated_no_response$STL_home, main = "QQplot of STL_Home")
qqline(home_team_stats_aggregated_no_response$STL_home)
  
qqnorm(home_team_stats_aggregated_no_response$BLK_home, main = "QQplot of BLK_Home")
qqline(home_team_stats_aggregated_no_response$BLK_home)

qqnorm(home_team_stats_aggregated_no_response$TO_home, main = "QQplot of TO_Home")
qqline(home_team_stats_aggregated_no_response$TO_home)

qqnorm(home_team_stats_aggregated_no_response$PF_home, main = "QQplot of PF_Home")
qqline(home_team_stats_aggregated_no_response$PF_home)

qqnorm(home_team_stats_aggregated_no_response$avg_PTS_per_player_home, main = "QQplot of Avg_PTS_per_player_Home")
qqline(home_team_stats_aggregated_no_response$avg_PTS_per_player_home)

```
  
For the most part, the data on these graphs neatly fall along the diagonal line. The plot of data for 'BLK_Home' is by far the least normally distributed, but because we have such a large sample size, there is still a high chance the errors will be distributed about 0 and follow a normal distribution.
There is no need to do transformations, like a log() transformation, to any of these variables.  

## Multivariate Analysis

### Pairplots to view normality

```{R} 

#Need to provide pair plots of the data to analyze the multivariate relationships 
par(fig.width = 10, fig.height = 20)
pairs(home_team_stats_aggregated_no_response, pch = ".", cex = 0.5)

``` 
The plots feature elliptical/circular shapes, therefore they do not reveal any strange patterns that might suggest non-normality.

### Correlation matrix to view relationships

```{R} 
# Calculate correlation matrix
correlation_matrix <- cor(home_team_stats_aggregated_no_response)

# Print the correlation matrix
print(correlation_matrix)

corrplot(correlation_matrix, type = "lower")
``` 
Through the correlation matrix, we can observe which variables have relationships. We can see that avg_PTS_per_player_home and FG_PCT_home have positive correlation with one another.  OREB_home and DREB_home are all positively correlated with REB_home. FG3_PCT_home and AST_home are correlated with FG_PCT_home. AST_home is also correlated with FG3_PCT_home. OREB_home and REB_home interestingly appear to be negatively correlated with FG_PCT_home. All the other correlations are weak and not worth pointing out.

### Dimensionality Reduction and Principal Component Analysis

```{R}
pca.fit.R <- prcomp(home_team_stats_aggregated_no_response, scale.= TRUE)
kable(round(pca.fit.R$rotation, 2))
```

```{R}
summary(pca.fit.R)
```

```{R}
par(mfrow=c(1,1), cex=0.5, mar=c(3.1,3.1,1,0.5),mgp=c(1.8,0.5,0), bty="L")
plot(pca.fit.R, type="l", main="Scree Plot")
```